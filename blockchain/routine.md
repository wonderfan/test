# Golang Routine

All of go routines are keeped recorde by allgs slice in the runtime. Some goroutines are active, some are IO waiting, channel blocking, scheduling or locking. The goroutine has a lot of properties which can help debug the application. The gid, atomic status and gopc are the basic properties. The waiting reason can help us to know why the goroutine is waiting. 

The g, m, p data model are defined in the runtime source file. The fields of g are the goroutine properties. Go has implemented different strategies on verticals like concurrency, system calls, task scheduling, and memory modeling, among others. A Goroutine is defined as a lightweight thread managed by the Go runtime. Different Goroutines (G) can be executed on different OS threads (M), but at any given time, only one OS thread can be run on a CPU (P). In the user space, you achieve concurrency as the Goroutines work cooperatively. In the presence of a blocking operation (network, I/O or system call), another Goroutine can be assigned to the OS thread.

Goroutines live within the user thread space. In comparison to OS threads, their operations cost less: The overhead for assigning them, suspending them, and resuming them is lower than the overhead required by OS threads. Goroutines and channels are two of the most important primitives Go offers for concurrency. One important aspect of Goroutines is that expressing them in terms of code is fairly easy. You simply put the keyword go before the function you want to schedule to be run outside of the main thread.

Go comes with its own runtime scheduler. The language does not rely on the native OS thread/process scheduler, but it cooperates with it. Because the scheduler is an independent component, it has the flexibility for implementing optimizations. All these optimizations aim for one thing: to avoid too much preemption of the OS Goroutines, which would result in suspending and resuming the functions’ execution, an expensive operation.

In order to properly prioritize your performance effort, the most valuable strategy is to identify your bottlenecks and focus on them. To achieve this, use profiling tools! Pprof and Trace are your friends. Goroutines is the feature that makes concurrency easy to implement in Golang. Though programmers choose to run the routines on a single core in most cases, there is a way to utilize more than one core to run the Goroutines. You can use “`GOMAXPROCS“` environment variable to set the number of cores that Goroutines uses. You need to choose this number wisely. If your Goroutines communicate a lot, it is better to be on one core. Else, using multiple cores can be advantageous.

Profilers do dynamic analysis of the program to measure the performance in many aspects, like CPU utilization and memory allocation. Profilers also point out the piece of code that is misbehaving: consuming a lot of the CPU’s capacity or making too many calls to a method. Library ants(https://github.com/panjf2000/ants) implements a goroutine pool with fixed capacity, managing and recycling a massive number of goroutines, allowing developers to limit the number of goroutines in your concurrent programs.

Go’s scheduler has three basic concepts. G is goroutine. M is an OS thread. P is a processor. P handles multiplexing some goroutines onto some OS threads. OS thread behaves like a worker for goroutine using a run queue. What is important is P uses a smart scheduling strategy called a work-stealing algorithm. Therefore P efficiently schedules available goroutines onto OS threads. 

## Networking

The network event has kqueue and epool mechanism for IO events. Go has a powerful built-in profiler that supports CPU, memory, goroutine and block (contention) profiling.Goroutine profile dumps the goroutine call stack and the number of running goroutines. Blocking profile shows function calls that led to blocking on synchronization primitives like mutexes and channels. A “Stop the World” (STW) is a crucial phase in some garbage collector algorithms to get track of the memory. It suspends the execution of the program to scan the memory roots and add write barriers. Stopping the program means stopping the running goroutines. The test, benchmark, profile and trace are important tools in the golang world. 

In many popular programming environments the stack usually refers to the call stack of a thread. A call stack is a LIFO stack data structure that stores arguments, local variables, and other data tracked as a thread executes functions. Each function call adds (pushes) a new frame to the stack, and each returning function removes (pops) from the stack.

Since threads are managed by the OS, the amount of memory available to a thread stack is typically fixed, e.g. a default of 8MB in many Linux environments. This means we also need to be mindful of how much data ends up on the stack, particularly in the case of deeply-nested recursive functions. If the stack pointer in the diagram above passes the stack guard, the program will crash with a stack overflow error.

The heap is a more complex area of memory that has no relation to the data structure of the same name. We can use the heap on demand to store data needed in our program. Memory allocated here can’t simply be freed when a function returns, and needs to be carefully managed to avoid leaks and fragmentation. The heap will generally grow many times larger than any thread stack, and the bulk of any optimization efforts will be spent investigating heap use.

Threads managed by the OS are completely abstracted away from us by the Go runtime, and we instead work with a new abstraction: goroutines. Goroutines are conceptually very similar to threads, but they exist within user space. This means the runtime, and not the OS, sets the rules of how stacks behave.

Go compilers will allocate variables that are local to a function in that function’s stack frame. However, if the compiler cannot prove that the variable is not referenced after the function returns, then the compiler must allocate the variable on the garbage-collected heap to avoid dangling pointer errors. Also, if a local variable is very large, it might make more sense to store it on the heap rather than the stack.

If a variable has its address taken, that variable is a candidate for allocation on the heap. However, a basic escape analysis recognizes some cases when such variables will not live past the return from the function and can reside on the stack. Stack traces play a critical role in Go profiling. Unwinding (or stack walking) is the process of collecting all the return addresses (elements in Stack Layout) from the stack. Frame pointer unwinding is the simple process of following the base pointer register (rbp) to the first frame pointer on the stack which points to the next frame pointer and so on. Symbolization is the process of taking one or more program counter (pc) address and turning them into human readable symbols such a function names, file names and line numbers. This article provides more details (https://github.com/DataDog/go-profiler-notes/blob/main/stack-traces.md).

## Perf command

perf_events, like other debug tools, needs symbol information (symbols). These are used to translate memory addresses into function and variable names, so that they can be read by us humans. Without symbols, you'll see hexadecimal numbers representing the memory addresses profiled.

## Go Scheduler

In Go and Golang programming, a scheduler is responsible for distributing jobs in a multiprocessing environment. When the available resources are limited, it is the task of the scheduler to manage the work that needs to be done in the most efficient way. In Go, the scheduler is responsible for scheduling goroutines, which is particularly useful in concurrency. Goroutines are like OS threads, but they are much lighter weight. However, goroutines always take the help of the underlying OS thread model and the scheduler it works on is at a much higher level than the OS scheduler. 

CPUs today come with multiple cores – these multicore processors are optimized to handle simultaneous execution – also known as parallel processing. This occurs at the hardware level and it is nice to have multiprocessing ability imbibed into the core functionality of the processors. But the problem is that there must be something that manages the incoming multiple jobs and distributes them among the available processors. This is the job of the scheduler and the process is known as scheduling. A scheduler schedules jobs at the software level and is a core part of the operating system functionality. Being part of the operating system, a scheduler is well aware of the intricacies and working mechanisms of the operating system; also, the scheduler must be aware of the hardware layout it is running on. This makes the scheduler a complex piece of software.

The Go runtime scheduler schedules goroutines. A goroutine is a lightweight thread that has the ability to execute on a single OS thread. The OS threads run on single or multiple available processors. The runtime scheduler of Go distributes goroutines over multiple threads. The scheduler determines the state of the goroutine. A life cycle of the goroutine can be in one of three fundamental states : running, runnable, and not runnable (due to IO blocked or system call).

Go works on a type of scheduler called an m:n scheduler (M:N scheduler), which states that M number of goroutines can be distributed over N number of OS threads. Comparatively, OS threads have much more overhead than goroutines. Therefore, Go uses a limited number of threads to run a maximum number of goroutines.

Similar to kernel level threads managed entirely by the OS, goroutines are user-space threads managed entirely by the Go runtime and the runtime scheduler schedules them. This makes goroutines cheaper, more lightweight than kernel threads, and they run on a very small memory footprint (with initial stack size of 2kb, whereas the default stack size of a thread is 8kb).

One of the problems with concurrent execution is the underutilized processor. Although the fair scheduling strategy tries to share execution load to all available processors, it is not always the case, because most distributed tasks are dependent on other tasks. This makes load sharing among multiple available processors unequal. There is always a chance that some processors are actually more utilized than the others. Moreover, holding a global lock to manage goroutines is expensive. Heavy IO block programs are prone to constant preemption of OS threads which is a significant overhead. A simple workaround of the problem is work stealing.

The work stealing strategy the Go scheduler looks for any logical underutilized processor and steals some processing time for the runnable goroutines to execute.

## Go Runtime

Within the Go source code, we can see the runtime source by looking at https://golang.org/src/runtime/. The runtime package contains operations that interact with the Go runtime. This package is used to control things such as goroutines, garbage collection, reflection, and scheduling, which are all functions that are essential to the operation of the language. Within the runtime package, we have many environment variables that help us change the runtime behavior of Go executables. 

GODEBUG is the controller of the variables and is used for debugging within the Go runtime. This variable contains a list of name=val key-value pairs, separated by commas. These named variables are used to tune the output of the debugging information the binary will return. One of the nice things about this variable is that the runtime allows you to apply this directly to a precompiled binary, rather than invoking it at build time. This is nice because it allows you to debug a binary that has already been built (and potentially already causing harm in a production environment). 

GCTRACE is utilized during runtime to view a single line that's been printed to stderr showing the total memory collected and the length of the pause during each collection. 

The GOGC variable allows us to tune the intensity of the Go garbage collection system. The garbage collector (instantiated at https://golang.org/src/runtime/mgc.go) reads the GOGC variable and determines the value of the garbage collector. A value of off sets the garbage collector off. This is often useful for debugging but not sustainable in the long term as the program needs to free memory that is collected within the executable's heap. Setting this value to less than the default value of 100 will cause the garbage collector to execute more frequently. Setting this value larger than the default value of 100 will cause the garbage collector to execute less frequently. Very often for multi-core large machines, garbage collection happens too frequently and we can improve performance if we have garbage collection happen less frequently. We can use a compilation of the standard library to see how changing garbage collection will influence compile times. 

GOMAXPROCS is a variable that can be tuned to allow us to control the number of threads that our operating system will allocate to our goroutines within our Go binary. By default, GOMAXPROCS is equal to the number of cores available to the application. This is dynamically configurable via the runtime package. It is important to note that as of Go 1.10, GOMAXPROCS will have no upper bound limit.

GOTRACEBACK allows you to control the generated output from a Go program with unexpected runtime conditions or unrecovered panic states. Setting a GOTRACEBACK variable will allow you to see more or less granular information about the goroutines that are instantiated for your specific error or panic.

The debug package within the runtime gives us many functions and types that are available for debugging. Package debug contains facilities for programs to debug themselves while they are running.
